<!DOCTYPE html>
<html lang="en">
<head>
<title>RGB LIDAR Fusion</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<style>
body,h1,h2,h3,h4,h5,h6 {font-family: "Lato", sans-serif}
.w3-bar,h1,button {font-family: "Montserrat", sans-serif}
.fa-anchor,.fa-coffee {font-size:200px}
</style>
</head>
<body>

<!-- Header -->
<header class="w3-container w3-red w3-center" style="padding:128px 16px">
  <h1 class="w3-margin w3-jumbo">RGB LIDAR Fusion</h1>
  <p class="w3-xlarge">Kelvin Kang, Calen Robinson, Jonathan Lord-Fonda, Gerald D'Ascoli</p>
  <a href="https://github.com/calenr/16824_RGB_LIDAR_Fusion" class="w3-button w3-black w3-padding-large w3-large w3-margin-top">Repository</a>

</header>

<!-- Motivation -->
<div class="w3-row-padding w3-padding-64 w3-container">
  <div class="w3-content">
    <div class="w3-twothird">
      <h1>Motivation</h1>

      <h5 class="w3-text-grey">Modern self-driving methods utilize a mixture of sensor modalities to ascertain the current situation in the environment.  This combination of sensors strengthens the overall system by covering the shortcomings of the individual sensors alone.  
	  For example, LIDAR information is accurate and consistent, but expensive and easily obfuscated by dust.  Meanwhile, image data is dense and cheap but is inconsistent and inaccurate.  Therefore, modern self-driving vehicles depend on the fusion of multiple modalities 
	  to ensure accuracy and consistency.  Training networks that can handle the information from multiple modalities, and even provide multiplicative strengths, is essential to improving solving self-driving.</h5>
    </div>

    <div class="w3-third w3-center">
	  <i class="fa fa-anchor w3-padding-64 w3-text-red"></i>
    </div>
  </div>
</div>

<!-- Prior Work -->
<div class="w3-row-padding w3-light-grey w3-padding-64 w3-container">
  <div class="w3-content">
    <div class="w3-third w3-center">
      <i class="fa fa-coffee w3-padding-64 w3-text-red w3-margin-right"></i>
    </div>

    <div class="w3-twothird">
      <h1>Prior Work</h1>
      <h5 class="w3-padding-32">
		[1] Fast and Accurate 3D Object Detection for Lidar-Camera-Based Autonomous Vehicles Using One Shared Voxel-Based Backbone
		<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9340187">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9340187</a>
		<br>
		[2] HDNET: Exploiting HD Maps for 3D Object Detection
		<a href="https://arxiv.org/pdf/2012.11704.pdf">https://arxiv.org/pdf/2012.11704.pdf</a>
		<br>
		[3] LiDAR-Camera Fusion for 3D Object Detection
		<a href="https://www.rit.edu/academicaffairs/facultyscholarship/submit/download_file.php?id=71433">https://www.rit.edu/academicaffairs/facultyscholarship/submit/download_file.php?id=71433</a>
		<br>
		[4] PointPainting: Sequential Fusion for 3D Object Detection
		<a href="https://arxiv.org/pdf/1911.10150.pdf">https://arxiv.org/pdf/1911.10150.pdf</a>
		<br>
		[5] High-level camera-LiDAR fusion for 3D object detection with machine learning
		<a href="https://arxiv.org/pdf/2105.11060.pdf">https://arxiv.org/pdf/2105.11060.pdf</a>
	  </h5>

      <h5 class="w3-text-grey">The first paper converts the Lidar information into voxels and superimposes it onto the camera images.  The second paper proposes a method for converting Lidar data into 3-d maps.  The third paper combines Lidar and camera information 
	  in a few different ways, using different architectures. The fourth paper projects lidar onto an image-segmentation output for embeddings. The fifth paper uses frustum region proposals generated by 2D object detectors to segment LiDAR point clouds into point 
	  clusters.</h5>
    </div>
  </div>
</div>

<!-- Your Idea -->
<div class="w3-row-padding w3-padding-64 w3-container">
  <div class="w3-content">
    <div class="w3-twothird">
      <h1>Our Idea</h1>
      <h5 class="w3-padding-32">What is the idea?</h5>
      <h5 class="w3-text-grey">The central idea is to fuse LiDAR and camera data together to produce robust 3D object detection (x, y, z, r, p, y) for use in self-driving applications. 3D object detection requires depth information which LiDAR can provide. 
	  However, the difficulty with working with LiDAR pointcloud is that they are large in size (~100MB per pointcloud) and are unstructured (as opposed to image data that can have constant pixel dimensions). Therefore, part of the work will be in finding 
	  a good way to encode the LiDAR data into a fixed feature space so that we can concatenate it with image embeddings to use Deep Learning models on it. In addition, we plan on using Resnet as a base line, then trying other architectures such as ConvNext 
	  or MobileNet if time allows.</h5>
	  
	  <h5 class="w3-padding-32">Why does your idea make sense intuitively?</h5>
      <h5 class="w3-text-grey"> This idea makes intuitive sense because it combines the strengths of different types of sensors. Lidars are useful for determining the precise 3D coordinates of objects in a scene, and cameras are good for classifying objects. 
	  Fusing these two sensors can provide robust classification and localization. However, fusing these two sensors requires a good representation of each. Cameras are straight forward, but lidar embeddings are still an area of active research. We hope to 
	  see how different embeddings affect the overall performance of the network.</h5>

	  <h5 class="w3-padding-32">How does it relate to prior work in the area?</h5>
      <h5 class="w3-text-grey"> We want to take the ideas of sensor fusion in 3D LiDAR-based datasets and apply new and improved object detection algorithms to see the performance. The prior work listed has provided various sensor fusion methods with various 
	  learning methods, we hope to build off of various successes found in their work to combine into a novel system for 3D object detection.</h5>
    </div>

    <div class="w3-third w3-center">
      <i class="fa fa-anchor w3-padding-64 w3-text-red"></i>
    </div>
  </div>
</div>
<div class="w3-row-padding w3-padding-64 w3-container">
	<div class="w3-content">
		<img src="assets/images/model_layout.png" alt="Model Layout" height="300" width="800">
	</div>
</div>

<!-- Results -->
<div class="w3-row-padding w3-light-grey w3-padding-64 w3-container">
  <div class="w3-content">
    <div class="w3-third w3-center">
      <i class="fa fa-coffee w3-padding-64 w3-text-red w3-margin-right"></i>
    </div>

    <div class="w3-twothird">
      <h1>Results</h1>
      <h5 class="w3-padding-32">Placeholder</h5>

      <p class="w3-text-grey">More placeholder</p>
    </div>
  </div>
</div>

<!-- Conclusion -->
<div class="w3-row-padding w3-padding-64 w3-container">
  <div class="w3-content">
    <div class="w3-twothird">
      <h1>Conclusion</h1>
      <h5 class="w3-padding-32">Not a placeholder</h5>

      <p class="w3-text-grey">Just kidding it is</p>
    </div>

    <div class="w3-third w3-center">
      <i class="fa fa-anchor w3-padding-64 w3-text-red"></i>
    </div>
  </div>
</div>

<!-- Future Directions -->
<div class="w3-row-padding w3-light-grey w3-padding-64 w3-container">
  <div class="w3-content">
    <div class="w3-third w3-center">
      <i class="fa fa-coffee w3-padding-64 w3-text-red w3-margin-right"></i>
    </div>

    <div class="w3-twothird">
      <h1>Future Directions</h1>
      <h5 class="w3-padding-32">Shrodinger's Placeholder</h5>

      <p class="w3-text-grey">You decide whether or not it's a placeholder</p>
    </div>
  </div>
</div>

</body>
</html>
